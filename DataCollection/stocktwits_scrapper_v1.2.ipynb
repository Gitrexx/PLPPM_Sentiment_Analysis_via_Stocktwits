{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd6d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import schedule\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9685f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTwitAPIScrapper:\n",
    "    def __init__(self):\n",
    "        self.intro()\n",
    "        self.url = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json?\"\n",
    "        self.maxId = None\n",
    "        self.symbol = None\n",
    "        self.lastRun = None\n",
    "        self.lastStatus = None\n",
    "        self.df = pd.DataFrame(\n",
    "            columns=['id', 'body', 'created_at', 'user', 'source', 'symbols', 'mentioned_users', 'entities'])\n",
    "        self.getInput()\n",
    "        self.req_count = 0\n",
    "\n",
    "    def intro(self):\n",
    "        message = 'for this scapper it scrap and output dataframe with 8 feilds: id, text body, created time, users, source, symbol, mentioned, entities, and currently only support input for 1 company symbol'\n",
    "        instruction = 'pls input some constrains for this scrapper'\n",
    "        print(message)\n",
    "        print(instruction)\n",
    "\n",
    "    def getInput(self):\n",
    "        self.maxId = input(\n",
    "            'by default scrapper start from current utc time backwards, press enter, else pls enter the max id for the data you want to scrap:')\n",
    "        while len(self.maxId) != 9 or not self.maxId.isdigit():\n",
    "            if self.maxId == '':\n",
    "                break\n",
    "            self.maxId = input('your max id is not valid, pls re-enter:')\n",
    "        self.symbol = input('pls enter the company symbol that you want to scrap:')\n",
    "        while requests.get(self.url.format(self.symbol)).status_code != 200:\n",
    "            self.symbol = input(\n",
    "                'seems there is no such company, pls re-enter the company symbol that you want to scrap:')\n",
    "\n",
    "    def scrap(self, continue_last_run=False, silent=False):\n",
    "\n",
    "        if self.lastRun is not None:\n",
    "            continue_last_run = True\n",
    "\n",
    "        if self.symbol == None:\n",
    "            print('you did not set the symbol, pls re-initialize a instance')\n",
    "\n",
    "        # number of queries to run for one file\n",
    "        query_times = 500\n",
    "\n",
    "        temp_url = self.url.format(self.symbol)\n",
    "\n",
    "        if not continue_last_run:\n",
    "            if self.maxId != None and self.maxId != '':\n",
    "                temp_url += 'max={}'.format(int(self.maxId) - 1)\n",
    "            self.df = pd.DataFrame(\n",
    "                columns=['id', 'body', 'created_at', 'user', 'source', 'symbols', 'mentioned_users', 'entities'])\n",
    "        else:\n",
    "            temp_url += 'max={}'.format(int(self.lastRun) - 1)\n",
    "\n",
    "        for i in range(query_times):\n",
    "            response = requests.get(temp_url)\n",
    "            messages = json.loads(response.content)['messages']\n",
    "            lastid = messages[-1]['id']\n",
    "            self.df = self.df.append(messages, ignore_index=True)\n",
    "            temp_url = self.url.format(self.symbol) + 'max={}'.format(int(lastid) - 1)\n",
    "            self.req_count += 1\n",
    "            print('num of rows for the current df is ', len(self.df.index))\n",
    "            if not silent:\n",
    "                print('run query {} time'.format(i + 1))\n",
    "            time.sleep(5)\n",
    "\n",
    "        self.lastRun = lastid\n",
    "        self.lastStatus = response.status_code\n",
    "        print('finished, {} queries in total this time'.format(query_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dba38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = StockTwitAPIScrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduled_scrap():\n",
    "    start_time = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
    "    # Change file output location if necessary\n",
    "    file_loc = r'C:\\Users\\Administrator\\Desktop\\output' + f'\\{app.symbol}_{start_time}.csv'\n",
    "    app.scrap(silent=True)\n",
    "    app.df.to_csv(file_loc)\n",
    "    time.sleep(10)\n",
    "    print('output to ', file_loc)\n",
    "    app.df = pd.DataFrame(\n",
    "        columns=['id', 'body', 'created_at', 'user', 'source', 'symbols', 'mentioned_users', 'entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run task every 15 seconds, starting 10 seconds from now.\n",
    "# it's the time interval between each execution, excluding the actual execution time\n",
    "schedule.every(5).seconds.do(scheduled_scrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8630c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all jobs. make sure only one job is here. if not, cancel all the jobs and rerun the schedule\n",
    "schedule.get_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18553942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check current request count\n",
    "app.req_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cdc63b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run for certain number of requests\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dc2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel all jobs\n",
    "# schedule.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ab023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear all variables\n",
    "# globals().clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
