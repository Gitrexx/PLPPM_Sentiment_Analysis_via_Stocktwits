{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLP_Project_BERT+CNN_V1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring BERT, sentence_BERT, and pt, tf BERT"
      ],
      "metadata": {
        "id": "4FyBgusu9l7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BERT"
      ],
      "metadata": {
        "id": "bfPbQ7ri9haJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_fEL1EdE5Ri"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel, BertModel\n",
        "\n",
        "# initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# initialize model and set it to evaluation mode coz we not gonna train it\n",
        "model_tf = TFBertModel.from_pretrained('bert-base-uncased', output_hidden_states=False)\n",
        "model_pt = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=False)\n",
        "model_pt.eval()"
      ],
      "metadata": {
        "id": "FZI916kdQ5PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sent1 = 'I like this movie very much.'\n",
        "sample_sent2 = 'I love the movie, it is awesome.'\n",
        "sample_sent3 = 'I actually prefer horror movie.'"
      ],
      "metadata": {
        "id": "iB-pTc-rbHPD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input_tf1 = tokenizer(sample_sent1, return_tensors='tf')\n",
        "encoded_input_tf2 = tokenizer(sample_sent2, return_tensors='tf')\n",
        "encoded_input_tf3 = tokenizer(sample_sent3, return_tensors='tf')\n",
        "# encoded_input_pt = tokenizer(sample_sent, return_tensors='pt')\n",
        "output_tf1 = model_tf(encoded_input_tf1)\n",
        "output_tf2 = model_tf(encoded_input_tf2)\n",
        "output_tf3 = model_tf(encoded_input_tf3)\n",
        "# output_pt = model_pt(**encoded_input_pt)"
      ],
      "metadata": {
        "id": "nj2rTlUidz87"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_tf_emb1 = tf.math.reduce_mean(output_tf1[0],axis=1)\n",
        "sample_tf_emb2 = tf.math.reduce_mean(output_tf2[0],axis=1)\n",
        "sample_tf_emb3 = tf.math.reduce_mean(output_tf3[0],axis=1)\n",
        "\n",
        "# This pooled_output from the BERT output = tf.layers.dense([cls]_token_embeddings_tensor, embed_size=768, activation=tanh,...)\n",
        "# is different from any other embed output, just as a representaion of this sentence regardless of padding, masking, etc.\n",
        "# sample_tf_emb1 = output_tf1[1]\n",
        "# sample_tf_emb2 = output_tf2[1]\n",
        "# sample_tf_emb3 = output_tf3[1]"
      ],
      "metadata": {
        "id": "ZA0QP0uCemXy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sentence BERT"
      ],
      "metadata": {
        "id": "gScEG0E55Zpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "EaiBuO9w5cJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "metadata": {
        "id": "d2JGnGZK5h7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_emb1 = sbert_model.encode([sample_sent1])\n",
        "sample_emb2 = sbert_model.encode([sample_sent2])\n",
        "sample_emb3 = sbert_model.encode([sample_sent3])"
      ],
      "metadata": {
        "id": "9DZzdGSH6FVp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_emb1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuCbER0KrOi-",
        "outputId": "bc9ba6a0-2044-4aba-cfba-3a0e2ba59474"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "print('different model on same sentence')\n",
        "print(1-cosine(sample_emb1[0],sample_tf_emb1[0]))\n",
        "print(1-cosine(sample_emb2[0],sample_tf_emb2[0]))\n",
        "print(1-cosine(sample_emb3[0],sample_tf_emb3[0]))\n",
        "# Make sense! different models output 768 dimensions all have different meaning, of course they can not be compared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e9CZZfc6G-u",
        "outputId": "c7202430-03c8-456e-8d3d-7f0b7ae86f13"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "different model on same sentence\n",
            "0.29537123441696167\n",
            "0.28814637660980225\n",
            "0.3951141834259033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('model performance for each model')\n",
        "print('sentenceBERT')\n",
        "print(sample_sent1,sample_sent2,1-cosine(sample_emb1[0],sample_emb2[0]))\n",
        "print(sample_sent1,sample_sent3,1-cosine(sample_emb1[0],sample_emb3[0]))\n",
        "print('BERT')\n",
        "print(sample_sent1,sample_sent2,1-cosine(sample_tf_emb1[0],sample_tf_emb2[0]))\n",
        "print(sample_sent1,sample_sent3,1-cosine(sample_tf_emb1[0],sample_tf_emb3[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoS5JAIFACXU",
        "outputId": "884b92e9-59a1-4c91-e173-ad039b30ed5c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model performance for each model\n",
            "sentenceBERT\n",
            "I like this movie very much. I love the movie, it is awesome. 0.935429036617279\n",
            "I like this movie very much. I actually prefer horror movie. 0.5741518139839172\n",
            "BERT\n",
            "I like this movie very much. I love the movie, it is awesome. 0.849561333656311\n",
            "I like this movie very much. I actually prefer horror movie. 0.7852483987808228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seems for sentence contextual similarity, sentence BERT is better."
      ],
      "metadata": {
        "id": "-iH5i1149TpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the subsequent CNN model"
      ],
      "metadata": {
        "id": "vbbUdmOd9Sw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras import layers,Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D,Reshape, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "id": "oQuQrfZJCAFy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DOC_PER_INSTANCE = 1000\n",
        "EMB_SIZE = 768"
      ],
      "metadata": {
        "id": "Co_6p-A3E3_v"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(1024, 3, activation='relu',input_shape=(DOC_PER_INSTANCE,EMB_SIZE)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6ACfgU8ED_B",
        "outputId": "22f11148-1222-4f5a-8f49-086de4db3071"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_1 (Conv1D)           (None, 998, 1024)         2360320   \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 1024)             0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 2050      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,362,370\n",
            "Trainable params: 2,362,370\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}